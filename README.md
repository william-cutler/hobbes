Heavily sampled from https://github.com/mees/calvin#readme
# Quick Start
To begin, clone this repository locally
```bash
$ git clone --recurse-submodules https://github.com/william-cutler/hobbes.git
$ export HOBBES_ROOT=$(pwd)/hobbes
```
Install requirements:
```bash
$ cd $HOBBES_ROOT
$ conda create -n hobbes_env python=3.8  # or use virtualenv
$ conda activate hobbes_env
$ sh install.sh
```
If problems are encountered installing the correct version of pytorch or pytorch with cuda, you may have to run the following command:
```bash
$ pip3 install torch==1.13.0+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
```

Download dataset (choose which split you want to download with the argument `D`, `ABC` or `ABCD`): \
For Hobbes, we use environment 'D'. If you want to get started without downloading the whole dataset, use the argument `debug` to download a small debug dataset (1.3 GB).
```bash
$ cd $HOBBES_ROOT/dataset
$ sh download_data.sh D | ABC | ABCD | debug
```

# Training Models
All models are found at and should be run from:

```bash
$ cd $HOBBES_ROOT/hobbes_agents/hobbes_models/models
```
##	Train Single-Task Imitation Learning Agent
To train a model using the default sensor inputs (which is just the RGB Static camera), first open the `single_task_imitator_training.py` file and change `HOBBES_DATASET_ROOT_PATH` to the absolute path of the dataset root. After this change, the model can simply be run using the following command.

```bash
$ python single_task_imitator_training.py
```

In order to modify which sensor inputs to use, simply change the parameters found on the first five lines of the `main()` method in this file. Any combination of static camera observations, gripper camera observations, and robot proprioceptive information can be used in the model, so long as at least one observation is passed to the model.

Evaluation of the model can be done as simply as running the evaluation file after changing the `HOBBES_DATASET_ROOT_PATH` in said file.

```bash
$ python single_task_imitator_eval.py
```

In order to load a pretrained model, all model parameters must match up to how they were trained. These parameters should be changed for evaluation of models trained using information different than the preset.

## Train Multi-Task Imitation Learning Agent
To train a multi-task imitation learning agent, again change the `HOBBES_DATASET_ROOT_PATH`. The model can then be run.

```bash
$ python multi_task_training.py
```

Running the model without changing any parameters will run pretraining on the entire dataset. Changing `finetune=True` will then run finetuning on the default set of tasks specified. Parameters for pretraining and finetuning can be found in the respective methods `main_pretrain()` and `main_finetune()`.

Evaluation is also very simple. Metrics and gifs can simply be generated by running the evaluation script.

```bash
$ python multi_task_eval.py
```

In the `main()` method, there are multiple parameters which can be changed. These enable custom GIF generation for a train and target trajectory, and generation of evaluation metrics to be saved as a .csv file and the corresponding GIFs.